from flask import Flask, render_template, request, jsonify
import os
from dotenv import load_dotenv
import re
import requests
import os
import re
import random
import requests
from flask import Flask, request, jsonify
from dotenv import load_dotenv

load_dotenv()
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")

# =============================
# M3: Rule-based Sentiment Config
# =============================
POSITIVE_KEYWORDS = [
    "ì¢‹ë‹¤", "ì¢‹ì•„ìš”", "ìµœê³ ", "ì¬ë°Œ", "ì¬ë¯¸", "ê°ë™", "ëŒ€ë°•",
    "í–‰ë³µ", "ì›ƒ", "ê°ì‚¬", "íë§", "ì‘ì›", "ì§±", "ë©‹ì§€", "ì‚¬ë‘"
]

NEGATIVE_KEYWORDS = [
    "ë³„ë¡œ", "ì‹«ë‹¤", "ìµœì•…", "ë¶ˆí¸", "ì‹¤ë§", "ì§œì¦",
    "í™”ë‚˜", "ì•ˆì¢‹", "ëª»í•˜", "ë¬¸ì œ", "ë…¸ì¼", "ìš•", "í—¬"
]

# =============================
# Env
# =============================
load_dotenv()
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")

# =============================
# Text Utils
# =============================
def preprocess_text(text: str) -> str:
    text = (text or "").lower()
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°(í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ)
    text = re.sub(r"[^a-z0-9ê°€-í£\s]", "", text)
    # ê³µë°± ì •ë¦¬
    return " ".join(text.split()).strip()

def analyze_sentiment(text: str) -> str:
    # ë„ˆë¬´ ì§§ìœ¼ë©´ neutral
    if not text or len(text.strip()) < 3:
        return "neutral"

    t = preprocess_text(text)

    pos = sum(1 for kw in POSITIVE_KEYWORDS if kw in t)
    neg = sum(1 for kw in NEGATIVE_KEYWORDS if kw in t)

    if pos > neg:
        return "positive"
    elif neg > pos:
        return "negative"
    return "neutral"

def analyze_comments(comments: list):
    results = []
    counts = {
        "positive": 0,
        "negative": 0,
        "neutral": 0
    }

    for c in comments:
        sentiment = analyze_sentiment(c["text"])
        counts[sentiment] += 1

        results.append({
            "text": c["text"],
            "sentiment": sentiment,
            "author": c.get("author"),
            "likeCount": c.get("likeCount"),
            "publishedAt": c.get("publishedAt")
        })

    total = sum(counts.values())

    return results, counts, total


    total = sum(counts.values())
    return results, counts, total

def make_summary(counts: dict) -> str:
    pos = counts.get("positive", 0)
    neg = counts.get("negative", 0)
    neu = counts.get("neutral", 0)

    if pos > neg and pos > neu:
        return "ì „ë°˜ì ìœ¼ë¡œ ë°˜ì‘ì´ ì¢‹ì€ ì˜ìƒì…ë‹ˆë‹¤."
    elif neg > pos and neg > neu:
        return "ë¶€ì •ì ì¸ ë°˜ì‘ì´ ë§ì€ ì˜ìƒì…ë‹ˆë‹¤."
    else:
        return "ë°˜ì‘ì´ ì—‡ê°ˆë¦¬ëŠ” ì˜ìƒì…ë‹ˆë‹¤."

# =============================
# YouTube Utils
# =============================
def extract_video_id(url: str):
    if not url:
        return None

    # 1) watch?v=VIDEOID
    m = re.search(r"[?&]v=([a-zA-Z0-9_-]{11})", url)
    if m:
        return m.group(1)

    # 2) youtu.be/VIDEOID
    m = re.search(r"youtu\.be/([a-zA-Z0-9_-]{11})", url)
    if m:
        return m.group(1)

    # 3) shorts/VIDEOID
    m = re.search(r"shorts/([a-zA-Z0-9_-]{11})", url)
    if m:
        return m.group(1)

    return None

def fetch_comments(video_id: str, max_comments: int):
    """
    YouTube Data API v3 - commentThreads.list
    ë°˜í™˜: ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ (ê° ëŒ“ê¸€ì— text/author/likeCount/publishedAt í¬í•¨)
    """
    comments = []
    page_token = None
    per_page = 100  # API max

    while len(comments) < max_comments:
        fetch_count = min(per_page, max_comments - len(comments))

        params = {
            "part": "snippet",
            "videoId": video_id,
            "key": YOUTUBE_API_KEY,
            "maxResults": fetch_count,
            "textFormat": "plainText",
        }
        if page_token:
            params["pageToken"] = page_token

        api_url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(api_url, params=params, timeout=20)

        if res.status_code != 200:
            raise RuntimeError(f"YouTube API error {res.status_code}: {res.text}")

        data = res.json()
        items = data.get("items", [])

        for item in items:
            top = item["snippet"]["topLevelComment"]["snippet"]
            comments.append({
                "text": top.get("textDisplay", ""),
                "author": top.get("authorDisplayName", ""),
                "likeCount": top.get("likeCount", 0),
                "publishedAt": top.get("publishedAt", ""),
            })

        page_token = data.get("nextPageToken")
        if not page_token:
            break

    return comments

# =============================
# Flask App
# =============================
app = Flask(__name__)

# =============================
# 1) ì–¸ì–´ë³„ í‚¤ì›Œë“œ (ìµœì†Œì„¸íŠ¸)
# =============================
POSITIVE_KEYWORDS_KO = ["ì¢‹ë‹¤", "ì¢‹ì•„ìš”", "ìµœê³ ", "ì¬ë°Œ", "ì¬ë¯¸", "ê°ë™", "ëŒ€ë°•", "í–‰ë³µ", "íë§", "ì‘ì›", "ì‚¬ë‘", "ë©‹ì§€", "ì§±"]
POSITIVE_KEYWORDS_KO += [
    "ê¿€ì¼", "ì¡´ì¼", "ê°œê¿€", "ë ˆì „ë“œ", "ê°“", "ì°¢ì—ˆ", "ë¯¸ì³¤ë‹¤",
    "ë¯¸ì³¤ë„¤", "ê°œì˜", "ì˜í•œë‹¤", "ìµœê³ ë‹¤", "ì‚¬ë‘í•´",
    "ë„ˆë¬´ ì¢‹", "ë„ˆë¬´ ì¬ë°Œ", "ì›ƒê¸°ë‹¤", "ê°ë™ì ", "ê°ì‚¬í•©ë‹ˆë‹¤"
]

@app.post("/api/analyze")
def analyze():
    data = request.get_json(silent=True) or {}
    url = (data.get("url") or "").strip()
    max_comments = data.get("maxComments")
NEGATIVE_KEYWORDS_KO = ["ë³„ë¡œ", "ì‹«ë‹¤", "ìµœì•…", "ë¶ˆí¸", "ì‹¤ë§", "ì§œì¦", "í™”ë‚˜", "ë…¸ì¼", "êµ¬ë¦¬", "ë§", "ìš•", "í—¬", "ë‹µë‹µ"]

POSITIVE_KEYWORDS_EN = ["good", "great", "awesome", "amazing", "love", "best", "fun", "cool", "perfect", "thanks", "helpful"]
NEGATIVE_KEYWORDS_EN = ["bad", "worst", "hate", "terrible", "awful", "boring", "trash", "annoying", "dislike", "cringe", "scam"]

POSITIVE_KEYWORDS_JA = ["æœ€é«˜", "å¥½ã", "ã„ã„", "è‰¯ã„", "é¢ç™½", "æ„Ÿå‹•", "ã™ã”ã„", "ã‚ã‚ŠãŒã¨ã†", "å¯æ„›ã„", "ç¥"]
NEGATIVE_KEYWORDS_JA = ["å«Œã„", "æœ€æ‚ª", "ã¤ã¾ã‚‰", "å¾®å¦™", "ã²ã©ã„", "ã‚´ãƒŸ", "ã†ã–ã„", "ç„¡ç†"]

POSITIVE_KEYWORDS_ZH = ["å¥½", "å¾ˆå¥½", "å¤ªæ£’", "å–œæ¬¢", "çˆ±", "ç²¾å½©", "æ„ŸåŠ¨", "å‰å®³", "è°¢è°¢", "å¯çˆ±"]
NEGATIVE_KEYWORDS_ZH = ["å·®", "å¾ˆå·®", "è®¨åŒ", "æœ€å·®", "æ— èŠ", "åƒåœ¾", "æ¶å¿ƒ", "ç³Ÿç³•", "å¤±æœ›"]

KEYWORDS = {
    "ko": (POSITIVE_KEYWORDS_KO, NEGATIVE_KEYWORDS_KO),
    "en": (POSITIVE_KEYWORDS_EN, NEGATIVE_KEYWORDS_EN),
    "ja": (POSITIVE_KEYWORDS_JA, NEGATIVE_KEYWORDS_JA),
    "zh": (POSITIVE_KEYWORDS_ZH, NEGATIVE_KEYWORDS_ZH),
}

MIN_LEN = 3

POSITIVE_EMOJIS = ["â¤ï¸", "ğŸ’•", "ğŸ’–", "ğŸ”¥", "ğŸ‘", "ğŸ‘", "ğŸ˜‚", "ğŸ¤£", "ğŸ¥¹"]
NEGATIVE_EMOJIS = ["ğŸ¤®", "ğŸ¤¢", "ğŸ˜¡", "ğŸ¤¬", "ğŸ‘"]

# =============================
# 2) ìœ íŠœë¸Œ URL â†’ videoId ì¶”ì¶œ
# =============================
def extract_video_id(url: str):
    if not url:
        return None

    m = re.search(r"youtu\.be/([A-Za-z0-9_-]{6,})", url)
    if m:
        return m.group(1)

    m = re.search(r"[?&]v=([A-Za-z0-9_-]{6,})", url)
    if m:
        return m.group(1)

    m = re.search(r"youtube\.com/shorts/([A-Za-z0-9_-]{6,})", url)
    if m:
        return m.group(1)

    m = re.search(r"youtube\.com/embed/([A-Za-z0-9_-]{6,})", url)
    if m:
        return m.group(1)

    return None

# =============================
# 3) lang=auto ê°ì§€
# =============================
RE_KO = re.compile(r"[ê°€-í£]")
RE_JA = re.compile(r"[\u3040-\u309F\u30A0-\u30FF]")
RE_ZH = re.compile(r"[\u4E00-\u9FFF]")
RE_EN = re.compile(r"[A-Za-z]")

def detect_lang_auto(text: str) -> str:
    if not text:
        return "en"
    if RE_KO.search(text):
        return "ko"
    if RE_JA.search(text):
        return "ja"
    zh_hits = len(RE_ZH.findall(text))
    en_hits = len(RE_EN.findall(text))
    if zh_hits >= 2 and en_hits == 0:
        return "zh"
    if en_hits > 0:
        return "en"
    return "en"

# =============================
# 4) ì „ì²˜ë¦¬ + ê°ì •ë¶„ì„
# =============================
def preprocess(text: str, lang: str) -> str:
    t = (text or "").strip().lower()

    if lang == "en":
        t = re.sub(r"[^a-z0-9\s]", " ", t)
    elif lang == "ko":
        t = re.sub(r"[^0-9a-zê°€-í£\s]", " ", t)
    else:  # ja/zh
        t = re.sub(r"[^\w\u3040-\u30FF\u4E00-\u9FFF\s]", " ", t)

    t = re.sub(r"\s+", " ", t).strip()
    return t

def score_keywords(cleaned: str, keywords: list[str]) -> int:
    return sum(1 for kw in keywords if kw and kw in cleaned)

def classify_sentiment(text: str, lang_choice: str):
    lang = detect_lang_auto(text) if lang_choice == "auto" else lang_choice
    cleaned = preprocess(text, lang)

    if len(cleaned) < MIN_LEN:
        return "neutral", lang

    pos_list, neg_list = KEYWORDS.get(lang, KEYWORDS["en"])
    pos = score_keywords(cleaned, pos_list)
    neg = score_keywords(cleaned, neg_list)

    # ì´ëª¨ì§€ ì ìˆ˜ ì¶”ê°€ (ì—¬ê¸° ë“¤ì—¬ì“°ê¸° ì¤‘ìš”!)
    for e in POSITIVE_EMOJIS:
        if e in text:
            pos += 1

    for e in NEGATIVE_EMOJIS:
        if e in text:
            neg += 1

    if pos > neg:
        return "positive", lang
    if neg > pos:
        return "negative", lang
    return "neutral", lang


def build_summary(ratios: dict) -> str:
    p = ratios.get("positive", 0.0)
    n = ratios.get("negative", 0.0)
    u = ratios.get("neutral", 0.0)

    top = max(p, n, u)
    close = sum(1 for x in (p, n, u) if abs(top - x) <= 0.10)

    if close >= 2:
        return "ë°˜ì‘ì´ ì—‡ê°ˆë¦¬ëŠ” ì˜ìƒì…ë‹ˆë‹¤."
    if top == p:
        return "ì „ë°˜ì ìœ¼ë¡œ ë°˜ì‘ì´ ì¢‹ì€ ì˜ìƒì…ë‹ˆë‹¤."
    if top == n:
        return "ë¶€ì •ì ì¸ ë°˜ì‘ì´ ë§ì€ ì˜ìƒì…ë‹ˆë‹¤."
    return "ì¤‘ë¦½ì ì¸ ë°˜ì‘ì´ ë§ì€ ì˜ìƒì…ë‹ˆë‹¤."

# =============================
# 5) YouTube API: ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
# =============================
def fetch_youtube_comments(video_id: str, max_comments: int, sort: str):
    if not YOUTUBE_API_KEY:
        raise RuntimeError("Missing YOUTUBE_API_KEY in .env")

    order_param = "time" if sort == "latest" else "relevance"

    comments = []
    page_token = None

    while len(comments) < max_comments:
        remaining = max_comments - len(comments)
        max_results = 100 if remaining > 100 else remaining

        params = {
            "part": "snippet",
            "videoId": video_id,
            "maxResults": max_results,
            "order": order_param,
            "textFormat": "plainText",
            "key": YOUTUBE_API_KEY,
        }
        if page_token:
            params["pageToken"] = page_token

        r = requests.get("https://www.googleapis.com/youtube/v3/commentThreads", params=params, timeout=15)
        if r.status_code != 200:
            raise RuntimeError(f"YouTube API error: {r.status_code} {r.text}")

        data = r.json()
        items = data.get("items", [])

        for it in items:
            sn = it["snippet"]["topLevelComment"]["snippet"]
            comments.append({
                "text": sn.get("textDisplay") or "",
                "likeCount": int(sn.get("likeCount") or 0),
                "publishedAt": sn.get("publishedAt") or "",
                "author": sn.get("authorDisplayName") or ""
            })

        page_token = data.get("nextPageToken")
        if not page_token or not items:
            break

    return comments

# =============================
# 6) ìš”ì²­ ê²€ì¦
# =============================
ALLOWED_MAX = {50, 100, 200}
ALLOWED_SORT = {"latest", "likes"}
ALLOWED_LANG = {"auto", "ko", "en", "ja", "zh"}

def bad_request(msg: str):
    return jsonify({"ok": False, "error": {"code": "BAD_REQUEST", "message": msg}}), 400

# =============================
# 7) POST /api/analyze
# =============================
@app.post("/api/analyze")
def api_analyze():
    try:
        data = request.get_json(force=True) or {}

        url = data.get("url")
        max_comments = data.get("maxComments")
        sort = data.get("sort", "latest")
        lang = data.get("lang", "auto")
        random_sample = bool(data.get("randomSample", False))

        if not url:
            return bad_request("url is required")

        if not isinstance(max_comments, int):
            return bad_request("maxComments must be an integer (50/100/200)")
        if max_comments not in ALLOWED_MAX:
            return bad_request("maxComments must be one of 50, 100, 200")

        if sort not in ALLOWED_SORT:
            return bad_request('sort must be "latest" or "likes"')
        if lang not in ALLOWED_LANG:
            return bad_request('lang must be "auto"|"ko"|"en"|"ja"|"zh"')

        video_id = extract_video_id(url)
        if not video_id:
            return bad_request("Could not extract videoId from url")

        raw_comments = fetch_youtube_comments(video_id, max_comments, sort)

        if random_sample:
            random.shuffle(raw_comments)

        if len(raw_comments) == 0:
            return jsonify({
                "ok": True,
                "meta": {"videoId": video_id},
                "counts": {"totalFetched": 0},
                "sentiment": {"positive": 0, "negative": 0, "neutral": 0},
                "ratios": {"positive": 0.0, "negative": 0.0, "neutral": 0.0},
                "summary": "ëŒ“ê¸€ì´ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "comments": []
            })

        stats = {"positive": 0, "negative": 0, "neutral": 0}
        labeled = []

        for c in raw_comments:
            text = c.get("text", "")
            sent, detected_lang = classify_sentiment(text, lang)
            stats[sent] += 1
            labeled.append({
                "text": text,
                "sentiment": sent,
                "lang": detected_lang,
                "likeCount": c.get("likeCount", 0),
                "publishedAt": c.get("publishedAt", ""),
                "author": c.get("author", "")
            })

        total = len(labeled)
        ratios = {
            "positive": round(stats["positive"] / total, 4),
            "negative": round(stats["negative"] / total, 4),
            "neutral": round(stats["neutral"] / total, 4),
        }

        return jsonify({
            "ok": True,
            "meta": {
                "videoId": video_id,
                "requested": {"maxComments": max_comments, "sort": sort, "lang": lang, "randomSample": random_sample},
                "youtubeOrder": "time" if sort == "latest" else "relevance"
            },
            "counts": {"totalFetched": total},
            "sentiment": stats,
            "ratios": ratios,
            "summary": build_summary(ratios),
            "comments": labeled
        })

    except Exception as e:
        return jsonify({"ok": False, "error": {"code": "INTERNAL_ERROR", "message": str(e)}}), 500
@app.get("/test")
def test_api():
    return jsonify({"message": "M4 server is working!"})
@app.get("/routes")
def routes():
    return jsonify(sorted([str(r) for r in app.url_map.iter_rules()]))
@app.get("/run")
def run_analyze_in_browser():
    # ì—¬ê¸° ì˜ìƒ URLì€ í…ŒìŠ¤íŠ¸ìš©(ë¦­ë¡¤)ë¡œ ê³ ì •
    payload = {
        "url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
        "maxComments": 50,
        "sort": "latest",
        "lang": "auto",
        "randomSample": False
    }

    # Flask ë‚´ë¶€ì—ì„œ POST ìš”ì²­ì„ ê°€ì§œë¡œ ë§Œë“¤ì–´ /api/analyze ì‹¤í–‰
    with app.test_client() as c:
        res = c.post("/api/analyze", json=payload)
        return (res.data, res.status_code, res.headers.items())

    if max_comments <= 0:
        return jsonify({"ok": False, "error": "maxComments must be >= 1"}), 400

    video_id = extract_video_id(url)
    if not video_id:
        return jsonify({"ok": False, "error": "Invalid YouTube URL"}), 400

    if not YOUTUBE_API_KEY:
        return jsonify({"ok": False, "error": "Missing YOUTUBE_API_KEY"}), 500

    # 1) ëŒ“ê¸€ ìˆ˜ì§‘
    try:
        comments = fetch_comments(video_id, max_comments)
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

    # 2) ëŒ“ê¸€ 0ê°œë©´ ë¶„ì„ ìƒëµ
    if len(comments) == 0:
        return jsonify({
            "ok": True,
            "counts": {"totalFetched": 0},
            "sentiment": {"positive": 0, "negative": 0, "neutral": 0},
            "summary": "ëŒ“ê¸€ì´ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "comments": []
        }), 200

    # 3) ê°ì • ë¶„ì„ + í†µê³„ + ìš”ì•½
    analyzed_comments, sentiment_counts, total = analyze_comments(comments)
    summary = make_summary(sentiment_counts)

    return jsonify({
        "ok": True,
        "counts": {"totalFetched": total},
        "sentiment": sentiment_counts,
        "summary": summary,
        "comments": analyzed_comments
    }), 200

if __name__ == "__main__":
    app.run(debug=True)

